from evaluate import load
import os
import matplotlib.pyplot as plt
from PIL import Image
import ast
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

meteor = load("meteor")
rouge = load("rouge")
bleu = load("bleu")

def compute_evaluation_metrics(generated_captions, reference_captions):
    """
    Compute METEOR and ROUGE scores for generated captions.

    Args:
        generated_captions (list of str): Captions generated by the model.
        reference_captions (list of list of str): Reference captions for each image.

    Returns:
        dict: Dictionary containing METEOR, BLEU and ROUGE scores.
    """
    # Load METEOR and ROUGE metrics
    # Compute METEOR score
    meteor_score = meteor.compute(predictions=generated_captions, references=reference_captions)

    # Compute ROUGE scores
    rouge_score = rouge.compute(predictions=generated_captions, references=reference_captions)

    bleu_score = bleu.compute(predictions=generated_captions, references=reference_captions)

    # Aggregate results
    results = {
        "meteor": meteor_score["meteor"],
        "bleu": bleu_score["bleu"],
        "rouge1": rouge_score["rouge1"],
        "rouge2": rouge_score["rouge2"],
        "rougeL": rouge_score["rougeL"],
    }

    return results


def f1(tp, tn, fp, fn):
    return {
        "Precision": tp/(tp+fp),
        "Recall": tp/(tp+fn),
        "F1-score": 2*tp/(2*tp+fp+fn)
    }