from evaluate import load
import os
import matplotlib.pyplot as plt
from PIL import Image
import ast
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

def compute_evaluation_metrics(generated_captions, reference_captions):
    """
    Compute METEOR and ROUGE scores for generated captions.

    Args:
        generated_captions (list of str): Captions generated by the model.
        reference_captions (list of list of str): Reference captions for each image.

    Returns:
        dict: Dictionary containing METEOR, BLEU and ROUGE scores.
    """
    meteor = load("meteor")
    rouge = load("rouge")

    meteor_score = meteor.compute(predictions=generated_captions, references=reference_captions)
    rouge_score = rouge.compute(predictions=generated_captions, references=reference_captions)

    gen_, ref_ = convert_captions(generated_captions, reference_captions):
    bleu2 = corpus_bleu(
        ref_,
        gen_
        hypotheses,
        weights=(0.5, 0.5, 0, 0),
    )
    bleu3 = corpus_bleu(
        ref_,
        gen_
        hypotheses,
        weights=(0.33, 0.33, 0.33, 0),
    )
    
    results = {
        "meteor": meteor_score["meteor"],
        "bleu2": bleu2,
        "bleu3": bleu3,
        "rouge1": rouge_score["rouge1"],
        "rouge2": rouge_score["rouge2"],
        "rougeL": rouge_score["rougeL"],
    }
    return results




# Function to display the original image with captions
def display_original_image_with_captions(dataset, idx, generated_captions, reference_captions):
    """
    Displays the original image along with its generated and reference captions.

    Args:
        dataset (Flickr8kDataset): The dataset instance.
        idx (int): Index of the image to display.
        generated_captions (list of str): List of captions generated by the model.
        reference_captions (list of list of str): List of reference captions for each image.
    """
    image_ids = dataset.get_image_ids()
    image_id = image_ids[idx]
    image_path = os.path.join(dataset.image_dir, image_id)
    
    original_image = Image.open(image_path).convert("RGB")
    
    plt.figure(figsize=(original_image.width / 100, original_image.height / 100))  # Scale figsize based on image size
    plt.imshow(original_image)
    plt.axis('off')  
    plt.show()



def convert_captions(generated_captions, reference_captions):
    """
    Convert generated and reference captions into the required formats for BLEU score calculation.

    Args:
        generated_captions (list of str): Captions generated by the model.
        reference_captions (list of list of str): Reference captions for each image.

    Returns:
        hypotheses (list of list of str): Generated captions as lists of words.
        list_of_references (list of list of list of str): Reference captions as lists of words.
    """
    # Initialize lists to store processed captions
    hypotheses = []
    list_of_references = []

    # Process generated captions
    for caption_str in generated_captions:
        # Convert string representation of list to actual list
        tokens = ast.literal_eval(caption_str)
        # Remove 'startseq' and 'endseq' tokens
        tokens = [token for token in tokens if token not in ('startseq', 'endseq')]
        hypotheses.append(tokens)

    # Process reference captions
    for refs in reference_captions:
        ref_list = []
        for ref_str in refs:
            # Convert string representation of list to actual list
            tokens = ast.literal_eval(ref_str)
            # Remove 'startseq' and 'endseq' tokens
            tokens = [token for token in tokens if token not in ('startseq', 'endseq')]
            ref_list.append(tokens)
        list_of_references.append(ref_list)

    return hypotheses, list_of_references
