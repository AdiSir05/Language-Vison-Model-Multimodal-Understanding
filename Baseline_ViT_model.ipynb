{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqlT-Bn-IZyL",
        "outputId": "29599bc6-5a52-4083-cff8-99099572bfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "\n",
        "from transformers import ViTModel, ViTConfig, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "import gdown\n",
        "\n",
        "# ----------------------------- #\n",
        "#         Configuration          #\n",
        "# ----------------------------- #\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract(url, output_zip, extract_to):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file from the given URL.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_zip):\n",
        "        print(\"Downloading dataset...\")\n",
        "        gdown.download(url, output_zip, quiet=False)\n",
        "    else:\n",
        "        print(f\"{output_zip} already exists. Skipping download.\")\n",
        "\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(\"Extracting dataset...\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Dataset extracted to: {extract_to}\")\n",
        "    else:\n",
        "        print(f\"{extract_to} already exists. Skipping extraction.\")\n",
        "\n",
        "# Flickr8k dataset Google Drive URL\n",
        "# Ensure this URL points to a zip file containing the 'flicker8k' folder with images and text files\n",
        "url = \"https://drive.google.com/uc?id=1iFgG55ZUR1ZO-BrIc5PQ1AhrWa0NhQVZ\"\n",
        "output_zip = \"flickr8k.zip\"\n",
        "extract_to = \"./flickr8k\"\n",
        "\n",
        "# Download and extract the dataset\n",
        "download_and_extract(url, output_zip, extract_to)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHXh4XUXI7QS",
        "outputId": "0c410477-7d09-4513-eeb2-2896e48f509d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1iFgG55ZUR1ZO-BrIc5PQ1AhrWa0NhQVZ\n",
            "From (redirected): https://drive.google.com/uc?id=1iFgG55ZUR1ZO-BrIc5PQ1AhrWa0NhQVZ&confirm=t&uuid=05f23842-c96b-4df4-a2c4-018aa89ebe06\n",
            "To: /content/flickr8k.zip\n",
            "100%|██████████| 1.12G/1.12G [00:08<00:00, 134MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset extracted to: ./flickr8k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8kDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Flickr8k Dataset for Image Captioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, captions_file, image_list_file, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load the image list\n",
        "        with open(image_list_file, \"r\") as file:\n",
        "            self.image_list = set(line.strip() for line in file.readlines())\n",
        "\n",
        "        # Load captions\n",
        "        self.image_captions = {}\n",
        "        with open(captions_file, \"r\") as file:\n",
        "            for line in file.readlines():\n",
        "                image_caption = line.strip().split(\"\\t\")\n",
        "                if len(image_caption) != 2:\n",
        "                    continue  # Skip malformed lines\n",
        "                image_id_caption_number = image_caption[0]\n",
        "                caption = image_caption[1].strip()\n",
        "                image_id = image_id_caption_number.split(\"#\")[0]\n",
        "                if image_id in self.image_list:\n",
        "                    if image_id in self.image_captions:\n",
        "                        self.image_captions[image_id].append(caption)\n",
        "                    else:\n",
        "                        self.image_captions[image_id] = [caption]\n",
        "\n",
        "        self.image_ids = list(self.image_captions.keys())\n",
        "        print(f\"Number of images in dataset: {len(self.image_ids)}\")\n",
        "\n",
        "    def get_image_ids(self):\n",
        "        return self.image_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_id)\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get a single caption (randomly selected)\n",
        "        captions = self.image_captions[image_id]\n",
        "        caption = random.choice(captions)\n",
        "\n",
        "        return image, caption  # Return a single caption"
      ],
      "metadata": {
        "id": "5kT5q0OIJC5K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "folder_path = os.path.join(extract_to, \"flicker8k\")\n",
        "image_dir = os.path.join(folder_path, \"images\")\n",
        "captions_file = os.path.join(folder_path, \"Flickr8k.token.txt\")\n",
        "train_list_file = os.path.join(folder_path, \"Flickr_8k.trainImages.txt\")\n",
        "test_list_file = os.path.join(folder_path, \"Flickr_8k.testImages.txt\")\n",
        "\n",
        "# Define image transformations\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Flickr8kDataset(image_dir, captions_file, train_list_file, transform)\n",
        "test_dataset = Flickr8kDataset(image_dir, captions_file, test_list_file, transform)\n",
        "\n",
        "# Define collate function\n",
        "def collate_fn(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    return {\"pixel_values\": images, \"captions\": captions}\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsWMwWsMJLE2",
        "outputId": "c0dcba18-d166-4089-dba0-f2c4fbe6e3f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in dataset: 6000\n",
            "Number of images in dataset: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTGPT2Captioner(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) + GPT-2 based Image Captioning Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vit_model_name='google/vit-base-patch16-224', gpt2_model_name='gpt2'):\n",
        "        super(ViTGPT2Captioner, self).__init__()\n",
        "\n",
        "        # Load pre-trained ViT model\n",
        "        print(\"Loading ViT model...\")\n",
        "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
        "        self.vit_config = ViTConfig.from_pretrained(vit_model_name)\n",
        "        self.vit_dim = self.vit_config.hidden_size  # Typically 768 for vit-base\n",
        "\n",
        "        # Load pre-trained GPT-2 model\n",
        "        print(\"Loading GPT-2 model...\")\n",
        "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
        "        self.gpt2.resize_token_embeddings(len(GPT2Tokenizer.from_pretrained(gpt2_model_name)))\n",
        "\n",
        "        # Project ViT output to GPT-2's embedding size if necessary\n",
        "        self.gpt2_embedding_dim = self.gpt2.config.n_embd  # Typically 768 for gpt2\n",
        "        if self.vit_dim != self.gpt2_embedding_dim:\n",
        "            self.vit_to_gpt2 = nn.Linear(self.vit_dim, self.gpt2_embedding_dim)\n",
        "            print(f\"Projecting ViT output from {self.vit_dim} to GPT-2 embedding size {self.gpt2_embedding_dim}\")\n",
        "        else:\n",
        "            self.vit_to_gpt2 = nn.Identity()\n",
        "            print(\"ViT and GPT-2 embedding dimensions match. No projection needed.\")\n",
        "\n",
        "        # Tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # Set pad token to eos_token\n",
        "\n",
        "    def forward(self, images, captions_input_ids, attention_mask=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "        \"\"\"\n",
        "        # Extract image features\n",
        "        vit_outputs = self.vit(images)\n",
        "        image_features = vit_outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
        "        image_features = self.vit_to_gpt2(image_features)  # [batch_size, gpt2_embedding_dim]\n",
        "\n",
        "        # Prepare GPT-2 inputs by prepending image features\n",
        "        # Expand image_features to sequence length 1\n",
        "        image_features = image_features.unsqueeze(1)  # [batch_size, 1, gpt2_embedding_dim]\n",
        "\n",
        "        # Get GPT-2 embeddings\n",
        "        gpt2_embeddings = self.gpt2.transformer.wte(captions_input_ids)  # [batch_size, seq_len, gpt2_embedding_dim]\n",
        "\n",
        "        # Concatenate image features and text embeddings\n",
        "        inputs_embeds = torch.cat([image_features, gpt2_embeddings], dim=1)  # [batch_size, 1 + seq_len, gpt2_embedding_dim]\n",
        "\n",
        "        # Adjust attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            image_attention_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "            attention_mask = torch.cat([image_attention_mask, attention_mask], dim=1)  # [batch_size, 1 + seq_len]\n",
        "\n",
        "        # Pass through GPT-2\n",
        "        outputs = self.gpt2(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "hw1hA_VJJMNN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTGPT2Captioner().to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 5\n",
        "\n",
        "# Function to save model checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"Checkpoint saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CabNKQJdJMQM",
        "outputId": "c7eab317-1188-466d-e621-6eb9f937fc73"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ViT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GPT-2 model...\n",
            "ViT and GPT-2 embedding dimensions match. No projection needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch}/{epochs}\"):\n",
        "        images = batch['pixel_values'].to(device)\n",
        "        captions = batch['captions']\n",
        "\n",
        "        # Tokenize captions\n",
        "        encoding = model.tokenizer(\n",
        "            captions,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=30\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        # Prepare labels by prepending -100 to ignore the image feature token\n",
        "        batch_size = input_ids.size(0)\n",
        "        padding = torch.full((batch_size, 1), -100, dtype=input_ids.dtype, device=device)\n",
        "        labels = torch.cat([padding, input_ids], dim=1)  # Shape: (batch_size, 1 + seq_len)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint after each epoch\n",
        "    checkpoint_filename = f\"vitgpt2_epoch{epoch}.pth\"\n",
        "    save_checkpoint(model, optimizer, epoch, avg_loss, checkpoint_filename)\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOsXkL3uJaaw",
        "outputId": "f457246d-736c-4b00-c51a-c2f917767991"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/5: 100%|██████████| 750/750 [04:59<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 2.1819\n",
            "Checkpoint saved to vitgpt2_epoch1.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/5: 100%|██████████| 750/750 [05:11<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Loss: 1.9058\n",
            "Checkpoint saved to vitgpt2_epoch2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/5: 100%|██████████| 750/750 [05:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Loss: 1.8263\n",
            "Checkpoint saved to vitgpt2_epoch3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/5: 100%|██████████| 750/750 [05:00<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Loss: 1.7317\n",
            "Checkpoint saved to vitgpt2_epoch4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/5: 100%|██████████| 750/750 [04:59<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Loss: 1.6594\n",
            "Checkpoint saved to vitgpt2_epoch5.pth\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Import the regular expressions module\n",
        "\n",
        "def generate_caption(model, image, tokenizer, device, max_length=30, sampling_strategy='greedy', top_k=50, top_p=0.95, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates a caption for a given image using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained ViT-GPT2 captioning model.\n",
        "        image (PIL.Image or Tensor): The input image.\n",
        "        tokenizer (GPT2Tokenizer): The tokenizer used during training.\n",
        "        device (torch.device): The device to perform computations on.\n",
        "        max_length (int): Maximum length of the generated caption.\n",
        "        sampling_strategy (str): Decoding strategy ('greedy', 'top_k', 'top_p').\n",
        "        top_k (int): Number of top tokens to consider for top-k sampling.\n",
        "        top_p (float): Cumulative probability threshold for top-p sampling.\n",
        "        temperature (float): Sampling temperature.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated caption.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Preprocess the image\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = image.convert(\"RGB\")\n",
        "            image = transform(image)  # Ensure 'transform' is defined globally\n",
        "        elif isinstance(image, torch.Tensor):\n",
        "            pass  # Image is already a tensor\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Provide a PIL.Image or torch.Tensor.\")\n",
        "\n",
        "        image = image.to(device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Extract image features using ViT\n",
        "        vit_outputs = model.vit(image)\n",
        "        image_features = vit_outputs.last_hidden_state[:, 0, :]  # [1, hidden_dim]\n",
        "        image_features = model.vit_to_gpt2(image_features)        # [1, gpt2_embedding_dim]\n",
        "        image_features = image_features.unsqueeze(1)             # [1, 1, gpt2_embedding_dim]\n",
        "\n",
        "        # Initialize generated tokens with a shorter prompt, e.g., \"Image:\"\n",
        "        prompt = \"A photo of\"\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)  # [1, len(prompt)]\n",
        "        attention_mask = torch.ones_like(input_ids).to(device)              # [1, len(prompt)]\n",
        "\n",
        "        generated_captions = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Get GPT-2 embeddings for the current input_ids\n",
        "            gpt2_embeddings = model.gpt2.transformer.wte(input_ids)  # [1, seq_len, gpt2_embedding_dim]\n",
        "\n",
        "            # Concatenate image features with text embeddings\n",
        "            inputs_embeds = torch.cat([image_features, gpt2_embeddings], dim=1)  # [1, 1 + seq_len, gpt2_embedding_dim]\n",
        "\n",
        "            # Update attention mask to account for image features\n",
        "            current_attention_mask = torch.cat([\n",
        "                torch.ones((1, image_features.size(1)), device=device),\n",
        "                attention_mask\n",
        "            ], dim=1)  # [1, 1 + seq_len]\n",
        "\n",
        "            # Forward pass through GPT-2\n",
        "            outputs = model.gpt2(inputs_embeds=inputs_embeds, attention_mask=current_attention_mask)\n",
        "            logits = outputs.logits  # [1, 1 + seq_len, vocab_size]\n",
        "\n",
        "            # Get the logits for the last token\n",
        "            next_token_logits = logits[:, -1, :]  # [1, vocab_size]\n",
        "\n",
        "            # Apply temperature\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            # Apply sampling strategy\n",
        "            if sampling_strategy == 'greedy':\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # [1, 1]\n",
        "            elif sampling_strategy == 'top_k':\n",
        "                probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "                top_k_probs, top_k_indices = torch.topk(probabilities, top_k, dim=-1)\n",
        "                top_k_probs = top_k_probs.squeeze(0)\n",
        "                top_k_indices = top_k_indices.squeeze(0)\n",
        "                next_token = torch.multinomial(top_k_probs, num_samples=1).unsqueeze(0)\n",
        "                next_token = top_k_indices[next_token]\n",
        "            elif sampling_strategy == 'top_p':\n",
        "                sorted_probs, sorted_indices = torch.sort(torch.softmax(next_token_logits, dim=-1), descending=True)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = 0  # Always keep the first token\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                next_token_logits[indices_to_remove] = -float('Inf')\n",
        "                probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported sampling strategy. Choose from 'greedy', 'top_k', 'top_p'.\")\n",
        "\n",
        "            # Decode the predicted token (for debugging purposes)\n",
        "            predicted_token = tokenizer.decode(next_token.squeeze(), skip_special_tokens=True)\n",
        "            # Uncomment the following lines for debugging\n",
        "            # print(f\"Predicted Token ID: {next_token.item()} | Token: {predicted_token}\")\n",
        "\n",
        "            # Append the predicted token to the generated caption\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break  # Stop generation if <EOS> token is predicted\n",
        "            generated_captions.append(predicted_token)\n",
        "\n",
        "            # Update input_ids and attention_mask for the next iteration\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)            # [1, seq_len + 1]\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=1)  # [1, seq_len + 1]\n",
        "\n",
        "        # Join all predicted tokens to form the final caption\n",
        "        caption = ' '.join(generated_captions).strip()\n",
        "\n",
        "        # Eliminate multiple consecutive spaces using regex\n",
        "        caption = re.sub(r'\\s+', ' ', caption)\n",
        "\n",
        "        # Optionally, capitalize the first letter and add a period at the end\n",
        "        caption = caption.capitalize()\n",
        "        if not caption.endswith('.'):\n",
        "            caption += '.'\n",
        "\n",
        "        return caption"
      ],
      "metadata": {
        "id": "tA0Hh_wiJahs"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate captions for the first 5 test images\n",
        "for i in range(5):\n",
        "    image, ref = test_dataset[i]\n",
        "    caption = generate_caption(model, image, model.tokenizer, device=device)\n",
        "    print(f\"Image {i+1} Caption: {caption}\")\n",
        "    print(f\"Image {i+1} Reference Caption: {ref}\")\n",
        "\n",
        "print(\"Caption generation completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecgICBPgJasn",
        "outputId": "cf33ad2d-20c7-4238-cd13-dbc2d45e0837"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1 Caption: A woman in a bikini is shown on a street .\n",
            "Image 1 Reference Caption: A woman is signaling is to traffic , as seen from behind .\n",
            "Image 2 Caption: A boy in a swimming pool .\n",
            "Image 2 Reference Caption: Children playing on the beach .\n",
            "Image 3 Caption: A man and a woman sitting on a bench .\n",
            "Image 3 Reference Caption: A man and a woman sitting on a dock .\n",
            "Image 4 Caption: A dog with a red collar is shown .\n",
            "Image 4 Reference Caption: A white dog is resting its head on a tiled floor with its eyes open .\n",
            "Image 5 Caption: A little boy with a red shirt and blue jeans .\n",
            "Image 5 Reference Caption: A boy with a toy gun .\n",
            "Caption generation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate rouge_score"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW8zFfw-Su5l",
        "outputId": "91b049f2-a897-4294-cf33-9e400e3c5f19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=39eeef5dd7e3437a1bc73ee1a1c961389b04e87672c18c7607db3a7566e64e4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "def compute_evaluation_metrics(generated_captions, reference_captions):\n",
        "    \"\"\"\n",
        "    Compute METEOR and ROUGE scores for generated captions.\n",
        "\n",
        "    Args:\n",
        "        generated_captions (list of str): Captions generated by the model.\n",
        "        reference_captions (list of list of str): Reference captions for each image.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing METEOR, BLEU and ROUGE scores.\n",
        "    \"\"\"\n",
        "    meteor = load(\"meteor\")\n",
        "    rouge = load(\"rouge\")\n",
        "    bleu = load(\"bleu\")\n",
        "\n",
        "    meteor_score = meteor.compute(predictions=generated_captions, references=reference_captions)\n",
        "    rouge_score = rouge.compute(predictions=generated_captions, references=reference_captions)\n",
        "    bleu_score = bleu.compute(predictions=generated_captions, references=reference_captions)\n",
        "\n",
        "    results = {\n",
        "        \"meteor\": meteor_score[\"meteor\"],\n",
        "        \"bleu\": bleu_score[\"bleu\"],\n",
        "        \"rouge1\": rouge_score[\"rouge1\"],\n",
        "        \"rouge2\": rouge_score[\"rouge2\"],\n",
        "        \"rougeL\": rouge_score[\"rougeL\"],\n",
        "    }\n",
        "    return results"
      ],
      "metadata": {
        "id": "SZQLWR74QLJ5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating captions for test images and evaluating...\")\n",
        "\n",
        "generated_captions = []\n",
        "reference_captions = []\n",
        "\n",
        "# Since test_dataset is set to return all captions, we can access them directly\n",
        "for idx in tqdm(range(len(test_dataset)), desc=\"Evaluating\"):\n",
        "    image, captions = test_dataset[idx]  # captions is a list of 5 reference captions\n",
        "    # Generate caption\n",
        "    generated_caption = generate_caption(model, image, model.tokenizer, device=device)\n",
        "    generated_captions.append(generated_caption)\n",
        "    reference_captions.append(captions)  # List of 5 reference captions\n",
        "\n",
        "# Compute evaluation metrics\n",
        "evaluation_results = compute_evaluation_metrics(generated_captions, reference_captions)\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"METEOR: {evaluation_results['meteor']:.4f}\")\n",
        "print(f\"ROUGE-1: {evaluation_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-L: {evaluation_results['rougeL']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or4LJZdjQMBB",
        "outputId": "232bc0bd-5f48-4ec2-d89a-21f9555e446f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating captions for test images and evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1000/1000 [02:09<00:00,  7.73it/s]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "METEOR: 0.2310\n",
            "ROUGE-1: 0.2972\n",
            "ROUGE-L: 0.2752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSA5wW0eSf09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}