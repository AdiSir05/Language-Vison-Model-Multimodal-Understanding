{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:10:27.912594Z",
     "start_time": "2024-12-13T07:10:20.819656Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.utils import save_image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "from score import f1\n",
    "import math\n",
    "from models.Wformer import *\n",
    "import os\n",
    "from utils import plot2images\n",
    "from utils import random_noise_composition\n",
    "from dataset import get_image_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbc24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "image_size = 256\n",
    "num_bits = 64\n",
    "batch_size = 64\n",
    "hidden_channels = 16\n",
    "num_fems = 5\n",
    "num_heads = 8\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540fe9f",
   "metadata": {},
   "source": [
    "# Train Wformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe2193ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = get_image_dataloader(\"./data/images/train\", transform=transform, batch_size=batch_size)\n",
    "val_loader = get_image_dataloader(\"./data/images/val\", transform=transform, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa334783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T07:59:31.498692Z",
     "start_time": "2024-12-13T07:59:31.193842Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 1e-3\n",
    "adv_lr = 1e-4\n",
    "image_loss_weight = 3\n",
    "wm_loss_weight = 10\n",
    "adv_steps = 3\n",
    "adv_loss_weight = 3\n",
    "\n",
    "\n",
    "encoder = Encoder(image_size, num_bits, num_fems, hidden_channels, num_heads).to(device)\n",
    "decoder = Decoder(image_size, num_bits, hidden_channels).to(device)\n",
    "discriminator = VisionTransformerClassifier(image_size, 2, 4, 2).to(device)\n",
    "\n",
    "encoder_opt = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_opt = optim.Adam(decoder.parameters(), lr=lr)\n",
    "discriminator_opt = optim.Adam(discriminator.parameters(), lr=adv_lr)\n",
    "image_crit = nn.MSELoss()\n",
    "wm_crit = nn.MSELoss()\n",
    "adv_crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652d21b8e7e885e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:06:06.737310Z",
     "start_time": "2024-12-13T08:04:35.717667Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_bit_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, images in enumerate(train_loader):\n",
    "        watermarks = torch.randint(0, 2, (images.shape[0], num_bits)).float().to(device)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "        \n",
    "        encoded_images = encoder(images, watermarks)\n",
    "        encoded_noised = random_noise_composition(encoded_images)\n",
    "        encoded_noised = encoded_images\n",
    "        images_noised = random_noise_composition(images)\n",
    "        decoded_watermarks_probs = decoder(encoded_noised.detach()) \n",
    "        decoded_watermarks = torch.round(decoded_watermarks_probs)\n",
    "        wm_loss = wm_crit(decoded_watermarks_probs, watermarks)\n",
    "        image_loss = image_crit(encoded_images, images)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(adv_steps):\n",
    "            discriminator_opt.zero_grad()\n",
    "            adv_original = discriminator(images.detach())\n",
    "            adv_original_noised = discriminator(images_noised.detach())\n",
    "            adv_encoded = discriminator(encoded_images.detach())\n",
    "            adv_encoded_noised = discriminator(encoded_noised.detach())\n",
    "            discriminator_loss = adv_crit(adv_original, torch.zeros(adv_original.shape[0]).to(device)) + \\\n",
    "                                        adv_crit(adv_original_noised, torch.zeros(adv_original.shape[0]).to(device)) + \\\n",
    "                                        adv_crit(adv_encoded_noised, torch.ones(adv_original.shape[0]).to(device)) + \\\n",
    "                                        adv_crit(adv_encoded_noised, torch.ones(adv_original.shape[0]).to(device))\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_opt.step()\n",
    "\n",
    "        \n",
    "        adv_encoded = discriminator(encoded_images)\n",
    "        adversary_loss =  adv_crit(adv_encoded, torch.ones(adv_original.shape[0]).to(device))\n",
    "        adv_scale = adv_loss_weight if avg_bit_acc > 0.9 else 0\n",
    "        loss = wm_loss_weight * wm_loss + image_loss_weight * image_loss + adv_loss_weight * adversary_loss\n",
    "        loss.backward()\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "    \n",
    "        \n",
    "        print('Epoch: {}/{}, Steps: {}/{},  Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "        print('Watermark Loss: {:.4f}'.format(wm_loss.item()))\n",
    "        print('Image Loss: {:.4f}'.format(image_loss.item()))\n",
    "        print(\"Discriminator Loss: {:.4f}\".format(discriminator_loss.item()))\n",
    "        print(\"Adversary Loss: {:.4f}\".format(adversary_loss.item()))\n",
    "        avg_bit_acc = torch.mean(torch.sum(decoded_watermarks == watermarks, dim=1).float() / num_bits)\n",
    "        print(f\"Avg bit accuracy: {avg_bit_acc}\")\n",
    "        \n",
    "    plot2images(encoded_noised[0], \"Encoded Noised\", encoded_images[0], \"Encoded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f8fabeb0343c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "avg_wm_loss = 0\n",
    "avg_image_loss = 0\n",
    "avg_discriminator_loss = 0\n",
    "avg_adv_loss = 0\n",
    "avg_bit_acc = 0\n",
    "avg_discriminator_acc = 0\n",
    "with torch.no_grad():\n",
    "     for i, images in enumerate(val_loader):\n",
    "         \n",
    "        watermarks = torch.randint(0, 2, (images.shape[0], num_bits)).float().to(device)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        encoded_images = encoder(images, watermarks)\n",
    "        decoded_watermarks_probs = decoder(encoded_images) \n",
    "        decoded_watermarks = torch.round(decoded_watermarks_probs)\n",
    "        \n",
    "        avg_wm_loss += wm_crit(decoded_watermarks_probs, watermarks)\n",
    "        avg_image_loss += image_crit(encoded_images, images)\n",
    "        adv_original = discriminator(images)\n",
    "        adv_encoded = discriminator(encoded_images)\n",
    "        avg_discriminator_acc += ((torch.argmax(adv_original, dim=-1) == 0).float().mean() + (torch.argmax(adv_encoded, dim=-1) == 1).float().mean()) / 2\n",
    "        avg_discriminator_loss += adv_crit(adv_original, torch.zeros(adv_original.shape[0]).to(device)) + adv_crit(adv_encoded, torch.ones(adv_original.shape[0]).to(device))\n",
    "        avg_adv_loss += adv_crit(adv_encoded, torch.ones(adv_encoded.shape[0]).to(device))\n",
    "        avg_bit_acc += torch.mean((decoded_watermarks == watermarks).float())\n",
    "\n",
    "print(f\"Avg bit accuracy: {avg_bit_acc/len(val_loader)}\")\n",
    "print(f\"Avg discriminator accuracy: {avg_discriminator_acc/len(val_loader)}\")\n",
    "print('Watermark Loss: {:.4f}'.format(avg_wm_loss.item()/len(val_loader)))\n",
    "print('Image Loss: {:.4f}'.format(avg_image_loss.item()/len(val_loader)))\n",
    "print(\"Discriminator Loss: {:.4f}\".format(avg_discriminator_loss.item()/len(val_loader)))\n",
    "print(\"Adversary Loss: {:.4f}\".format(avg_adv_loss.item()/len(val_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"./models/wformer_encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"./models/wformer_decoder.pth\")\n",
    "torch.save(discriminator.state_dict(), \"./models/vit_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d95ba1",
   "metadata": {},
   "source": [
    "# Use Wformer and ViT Classifier to Analyze Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f917aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/nvxc4hhs37705ll5k416pcy00000gn/T/ipykernel_28777/2476031497.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"./models/wformer_encoder.pth\", map_location=device))\n",
      "/var/folders/qq/nvxc4hhs37705ll5k416pcy00000gn/T/ipykernel_28777/2476031497.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(\"./models/wformer_decoder.pth\", map_location=device))\n",
      "/var/folders/qq/nvxc4hhs37705ll5k416pcy00000gn/T/ipykernel_28777/2476031497.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  discriminator.load_state_dict(torch.load(\"./models/vit_discriminator.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(image_size, num_bits, num_fems, hidden_channels, num_heads).to(device)\n",
    "decoder = Decoder(image_size, num_bits, hidden_channels).to(device)\n",
    "discriminator = VisionTransformerClassifier(image_size, 2, 4, 2).to(device)\n",
    "encoder.load_state_dict(torch.load(\"./models/wformer_encoder.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"./models/wformer_decoder.pth\", map_location=device))\n",
    "discriminator.load_state_dict(torch.load(\"./models/vit_discriminator.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bfcd34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.9765624996276856, 'Recall': 0.9999999996000001, 'F1-score': 0.9881422921042352, 'Accuracy': 0.9879999996095999}\n",
      "0.9983426630496979\n"
     ]
    }
   ],
   "source": [
    "tp = 1e-7\n",
    "fp = 1e-7\n",
    "tn = 1e-7\n",
    "fn = 1e-7\n",
    "avg_bit_acc = 0\n",
    "with torch.no_grad():\n",
    "    for images in val_loader:\n",
    "        images = images.to(device)\n",
    "        watermarks = torch.randint(0, 2, (images.shape[0], num_bits)).float().to(device)\n",
    "        encode_split = watermarks.shape[0]//2\n",
    "        watermarks[:encode_split] = 2\n",
    "        true_labels = (watermarks[:, 0] != 2).int()\n",
    "        images[encode_split:] = encoder(images[encode_split:], watermarks[encode_split:])\n",
    "        pred_labels = torch.argmax(discriminator(images), dim=-1)\n",
    "        tp += torch.sum((pred_labels == 1) & (pred_labels == true_labels)).item()\n",
    "        tn += torch.sum((pred_labels == 0) & (pred_labels == true_labels)).item()\n",
    "        fp += torch.sum((pred_labels == 1) & (pred_labels != true_labels)).item()\n",
    "        fn += torch.sum((pred_labels == 0) & (pred_labels != true_labels)).item()\n",
    "        if torch.sum(true_labels == 1) > 0:\n",
    "            encoded_images = images[true_labels == 1]\n",
    "            true_watermarks = watermarks[true_labels == 1]\n",
    "            pred_watermarks = torch.round(decoder(encoded_images))\n",
    "            avg_bit_acc += torch.mean((pred_watermarks == true_watermarks).float()).item()\n",
    "    print(f1(tp, tn, fp, fn))\n",
    "    print(avg_bit_acc / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2d82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
